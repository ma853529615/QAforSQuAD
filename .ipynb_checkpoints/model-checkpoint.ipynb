{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import string\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    SQuADpath=\"data/SQuAd/BatchSQuAD\",\n",
    "    train_proportion=0.7,\n",
    "    val_proportion=0.15,\n",
    "    test_proportion=0.15,\n",
    "    seed=1337\n",
    ")\n",
    "\n",
    "np.random.seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \n",
    "\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        \n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "    def to_serializable(self):\n",
    "        \n",
    "        return {'token_to_idx': self._token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "            \n",
    "    def add_many(self, tokens):\n",
    "        \n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):    #<END> TOKEN 和 <BEGIN> TOKEN的作用是？ 在非生成模型中有用吗？\n",
    "        \n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                         'mask_token': self._mask_token,\n",
    "                         'begin_seq_token': self._begin_seq_token,\n",
    "                         'end_seq_token': self._end_seq_token})\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectovizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer(object):\n",
    "    def __init__(self, question_vocab, passage_vocab, max_question_length, max_passage_length):\n",
    "        self.question_vocab = question_vocab\n",
    "        self.passage_vocab = passage_vocab\n",
    "        self.max_question_length = max_question_length\n",
    "        self.max_passage_length = max_passage_length\n",
    "        \n",
    "    def _vectorize(self, indices, vector_length=-1, mask_index=0):\n",
    "        \n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "        \n",
    "        vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        vector[:len(indices)] = indices\n",
    "        vector[len(indices):] = mask_index\n",
    "\n",
    "        return vector \n",
    "\n",
    "    def _get_question_indices(self, text):\n",
    "        \n",
    "        indices = [self.question_vocab.begin_seq_index]\n",
    "        indices.extend([self.question_vocab.lookup_token(token) for token in text])\n",
    "        indices.append(self.question_vocab.begin_seq_index)\n",
    "        \n",
    "        return indices\n",
    "    \n",
    "    def _get_passage_indices(self, text):\n",
    "        \n",
    "        indices = [self.passage_vocab.begin_seq_index]\n",
    "        indices.extend([self.passage_vocab.lookup_token(token) for token in text])\n",
    "        indices.append(self.passage_vocab.begin_seq_index)\n",
    "        \n",
    "        return indices\n",
    "    \n",
    "    def vectorize(self, question, passage, use_dataset_max_lengths=True):\n",
    "        \n",
    "        question_vector_length = -1\n",
    "        passage_vector_length = -1\n",
    "        \n",
    "        if use_dataset_max_lengths:\n",
    "            question_vector_length = self.max_question_length\n",
    "            passage_vector_length = self.max_passage_length\n",
    "            \n",
    "        question_indices = self._get_question_indices(question)\n",
    "        question_vector = self._vectorize(question_indices, \n",
    "                                        vector_length=question_vector_length, \n",
    "                                        mask_index=self.question_vocab.mask_index)\n",
    "        \n",
    "        passage_indices = self._get_passage_indices(passage)\n",
    "        \n",
    "        passage_vector = self._vectorize(passage_indices,\n",
    "                                         vector_length=passage_vector_length,\n",
    "                                        mask_index=self.passage_vocab.mask_index)\n",
    "        return {\"question_vector\": question_vector, \n",
    "                \"passage_vector\": passage_vector, \n",
    "                \"question_length\": len(question_indices),\n",
    "               \"passage_length\": len(passage_indices)} #实际长度\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, bitext_df, context, cutof=5): \n",
    "        \"\"\"\n",
    "        词频passage和question放在一起计算来cutof,\n",
    "        vocab是分开放的，embedding也是分开的，所以训练也是分开的\n",
    "        \"\"\"\n",
    "        \n",
    "        question_vocab = SequenceVocabulary()\n",
    "        passage_vocab = SequenceVocabulary()\n",
    "        \n",
    "        max_question_length = 0\n",
    "        max_passage_length = 0\n",
    "        sum_word = []\n",
    "        \n",
    "        for question in bitext_df.question:\n",
    "            sum_word.extend(question)\n",
    "        \n",
    "        for text in context.passage:\n",
    "            sum_word.extend(text)\n",
    "        \n",
    "        word_freq = Counter(sum_word) \n",
    "        \n",
    "        for question in bitext_df.question:\n",
    "            if len(question) > max_question_length:\n",
    "                max_question_length = len(question)\n",
    "            for word in question:\n",
    "                if word_freq[word] >= cutof:\n",
    "                    question_vocab.add_token(word)\n",
    "        \n",
    "        for text in context.passage:\n",
    "            if len(text) > max_passage_length:\n",
    "                max_passage_length = len(text)\n",
    "            for word in text:\n",
    "                if word_freq[word] >= cutof:\n",
    "                    passage_vocab.add_token(word)\n",
    "        \n",
    "        return cls(question_vocab, passage_vocab, max_question_length, max_passage_length)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        title_vocab = SequenceVocabulary.from_serializable(contents['question_vocab'])\n",
    "        category_vocab = Vocabulary.from_serializable(contents['passage_vocab'])\n",
    "\n",
    "        return cls(title_vocab=title_vocab, category_vocab=category_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'question_vocab': self.question_vocab.to_serializable(),\n",
    "                'passage_vocab': self.passage_vocab.to_serializable()}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_df, vectorizer):\n",
    "        self.data_df = data_df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        self.train_df = self.data_df[self.data_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.data_df[self.data_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.data_df[self.data_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "        \n",
    "    @classmethod\n",
    "    def load_from_df(cls, data_df, vec):\n",
    "        \n",
    "        train_subset = data_df[data_df.split=='train']\n",
    "        return cls(train_subset, vec)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, data_df, vectorizer_filepath):\n",
    "        \n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(data_df, vectorizer)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return Vectorizer.from_serializable(json.load(fp))\n",
    "        \n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "            \n",
    "    def get_vectorizer(self):\n",
    "     \n",
    "        return self._vectorizer\n",
    "    \n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        vector_dict = self._vectorizer.vectorize(row.question, row.context)\n",
    "\n",
    "        return {\"question\": vector_dict[\"question_vector\"], \n",
    "                \"passage\": vector_dict[\"passage_vector\"], \n",
    "                \"question_length\": vector_dict[\"question_length\"],\n",
    "                \"passage_length\": vector_dict[\"passage_length\"],\n",
    "                \"answer_start\": row.answer_start,\n",
    "                \"none_answer\": row.is_impossible}\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \n",
    "        return len(self)//batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MJHQAmodel(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, embedding1, embedding2):\n",
    "        \n",
    "        super(MJHQAmodel, self).__init__()\n",
    "        self.ques_embedding = nn.Embedding.from_pretrained(embedding1, padding_idx=0)\n",
    "        self.pas_embedding = nn.Embedding.from_pretrained(embedding2, padding_idx=0)\n",
    "        self.encoder = nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, bias=True, batch_first=True, bidirectional=True)\n",
    "        self.att_matrix = nn.Linear(2*hidden_size, 2*hidden_size)\n",
    "        \n",
    "    def forward(self, question, passage, question_length, passage_length):\n",
    "        \n",
    "        question = self.ques_embedding(question).float()\n",
    "        passage = self.pas_embedding(passage).float()\n",
    "        question = nn.utils.rnn.pack_padded_sequence(question, question_length, batch_first=True,enforce_sorted=False)\n",
    "        passage = nn.utils.rnn.pack_padded_sequence(passage, passage_length, batch_first=True, enforce_sorted=False)\n",
    "        hidden_state, (h_n, c_n) = self.encoder(question)\n",
    "        qas_represent = torch.cat((h_n, c_n), 2)\n",
    "        #qas_represent = nn.utils.rnn.pad_packed_sequence(qas_represent)\n",
    "        qas_represent = torch.unsqueeze(qas_represent, 2)\n",
    "        hidden_state, (h_n, c_n) = self.encoder(passage) #hidden_state待处理 \n",
    "        hidden_state = nn.utils.rnn.pad_packed_sequence(hidden_state)\n",
    "        print(hidden_state) #双向 待处理\n",
    "        att = self.att_matrix(hidden_state) #X\n",
    "        scores = torch.sum(torch.bmm(att, qas_represent), 2)\n",
    "        indice = torch.argmax(scores, dim=1)\n",
    "        \n",
    "        return indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nmt_batches(dataset, batch_size, shuffle=True, \n",
    "                            drop_last=True, device=\"cpu\"):\n",
    "    \n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        lengths = data_dict['question_length'].numpy()\n",
    "        sorted_length_indices = lengths.argsort()[::-1].tolist()\n",
    "        \n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name][sorted_length_indices].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2list(text_df, column=0):\n",
    "    \n",
    "    text_df.loc[:,column] = text_df.loc[:,column].apply(word_tokenize)\n",
    "    \n",
    "def mungdata(path, data_batch=10):\n",
    "    \"\"\"\n",
    "    passage单独另外放一起，避免重复计算词频\n",
    "    \"\"\"\n",
    "    QADataset = []\n",
    "    passage = []\n",
    "    for i in tqdm_notebook(range(data_batch)):\n",
    "        with open(path+\"/batch_{}.json\".format(i+1)) as fp:\n",
    "            batch = json.load(fp)\n",
    "            for para in tqdm_notebook(batch):\n",
    "                title = para['title']\n",
    "                for par in para['paragraphs']:\n",
    "                    text = {}\n",
    "                    text['passage'] = word_tokenize(par['context'].lower())\n",
    "                    passage.append(text)\n",
    "                    for question in par['qas']:\n",
    "                        sample = {}\n",
    "                        sample['title'] =  title\n",
    "                        sample['context'] = word_tokenize(par['context'].lower())\n",
    "                        sample['question'] = word_tokenize(question['question'].lower())\n",
    "                        sample['is_impossible'] = question['is_impossible']\n",
    "                        if not question['is_impossible']:\n",
    "                            sample['answer_start'] = question['answers'][0]['answer_start']\n",
    "                            sample['answer_text'] = question['answers'][0]['text']\n",
    "                        else:\n",
    "                            sample['answer_start'] = -1\n",
    "                            sample['answer_text'] = ''\n",
    "                        QADataset.append(sample)\n",
    "    QADataset = pd.DataFrame(QADataset)\n",
    "    QADataset['split'] = 0\n",
    "    passage = pd.DataFrame(passage)\n",
    "   #由Title分层采样  \n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=args.test_proportion+args.val_proportion, random_state=args.seed)\n",
    "\n",
    "    for train_index, test_index in split.split(QADataset, QADataset.title):\n",
    "        QADataset.iloc[train_index,-1] = 'train'#修改值的常规方式，其他方式可能会在copy上修改，没用\n",
    "        QADataset.iloc[test_index,-1] = 'test'\n",
    "\n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=args.seed)\n",
    "\n",
    "    for _, test_index in split.split(QADataset[QADataset.split=='train'], QADataset[QADataset.split=='train'].title):\n",
    "        QADataset.iloc[test_index,-1] = 'val'\n",
    "        \n",
    "    return pd.DataFrame(QADataset), pd.DataFrame(passage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLOVE(object):\n",
    "    def __init__(self, dic):\n",
    "        \n",
    "        self.word_embedding = dic\n",
    "        self.size = len(dic)\n",
    "        \n",
    "    @classmethod\n",
    "    def loaddic(cls, path):\n",
    "        \n",
    "        with open(path, encoding=\"utf-8\") as fp:\n",
    "            GLoVe = {}\n",
    "            for word_e in tqdm_notebook(fp.readlines()):\n",
    "                word_e = word_e.split(\" \")\n",
    "                GLoVe[word_e[0]] = [float(x) for x in word_e[1:]]\n",
    "            \n",
    "        return cls(GLoVe)\n",
    "    \n",
    "    def dic2matrix(self, vocab):\n",
    "        \"\"\"\n",
    "        因为array没有append方法，1.所以转成list再转回来\n",
    "        2.也可以用np.append(好像有点不太对,注意要输入axis)\n",
    "        \"\"\"\n",
    "        matrix = list(np.random.randn(4,50))\n",
    "        for idx in range(4, len(vocab)):\n",
    "            if vocab.lookup_index(idx) in self.word_embedding:\n",
    "                matrix.append(np.array(self.word_embedding[vocab.lookup_index(idx)]))\n",
    "            else:\n",
    "                matrix.append(np.random.randn(50))\n",
    "        return np.array(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346bc0b780e44782b8b35dff13cb8672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=400000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Glove = GLOVE.loaddic('data/GLoVe/glove.6B.50d.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77fef10afc7d454a909e41b8ca225d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe7659048a444da83960a0ecb09eeca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=45), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Vectorize = Vectorizer.from_dataframe(*mungdata('data/SQuAd/BatchSQuAD', data_batch=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.70318731, -0.49028236, -0.32181433, ..., -1.04630036,\n",
       "         0.13988892, -1.73065584],\n",
       "       [-0.13062312, -1.31026002, -2.17131242, ...,  0.33035865,\n",
       "        -0.54251518, -1.05202857],\n",
       "       [-0.77572065, -0.12322816, -0.53693127, ...,  0.8159017 ,\n",
       "        -0.50309222,  0.1448642 ],\n",
       "       ...,\n",
       "       [-0.51983   , -0.28089   , -1.594     , ..., -0.12471   ,\n",
       "         0.21602   , -0.21437   ],\n",
       "       [ 0.29329   , -0.49989   , -0.17533   , ..., -0.54471   ,\n",
       "        -0.22229   ,  0.55851   ],\n",
       "       [ 0.72141   , -0.15303   , -1.3826    , ...,  0.17129   ,\n",
       "         0.24375   , -0.34653   ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Glove.dic2matrix(Vectorize.passage_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212cc890b02047ff99ea89c085a09101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f7d8d8a22e478e91e1f3a28f9f8d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=45), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = QADataset.load_from_df(mungdata('data/SQuAd/BatchSQuAD', data_batch=1)[0], Vectorize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #有一个问题 就是预训练的weights怎么放进模型里面 有何种方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generater(dataset, batch_size, shuffle=True): #torch的dataloader不好用，不能带bool，自己实现吧\n",
    "        \n",
    "        batch_num = math.ceil(len(dataset) / batch_size)\n",
    "        index_array = list(range(len(dataset)))\n",
    "    \n",
    "        if shuffle:\n",
    "            np.random.shuffle(index_array)\n",
    "    \n",
    "        for i in range(batch_num):\n",
    "            indices = index_array[i * batch_size: (i + 1) * batch_size]\n",
    "            examples = [dataset[i] for i in indices]\n",
    "    \n",
    "            examples = sorted(examples, key=lambda e: len(e['question']), reverse=True)\n",
    "            examples = pd.DataFrame(examples)\n",
    "            yield {\"question\":torch.tensor(examples.question),\n",
    "                  \"passage\":torch.tensor(examples.passage),\n",
    "                   \"question_length\":torch.tensor(examples.question_length),\n",
    "                   \"passage_length\":torch.tensor(examples.passage_length),\n",
    "                  \"answer_start\":torch.tensor(examples.answer_start),\n",
    "                  \"noanswer\":torch.tensor(examples.none_answer)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaa\n",
      "(tensor([[[ 7.9590e-02, -1.3637e-01, -8.5280e-02,  ...,  1.6550e-01,\n",
      "          -2.0702e-03, -2.8041e-01],\n",
      "         [ 7.9590e-02, -1.3637e-01, -8.5280e-02,  ..., -5.3330e-02,\n",
      "           8.9291e-02, -1.8827e-01],\n",
      "         [ 7.9590e-02, -1.3637e-01, -8.5280e-02,  ..., -3.1181e-04,\n",
      "           4.8611e-02, -1.3884e-01],\n",
      "         [ 7.9590e-02, -1.3637e-01, -8.5280e-02,  ..., -3.0305e-03,\n",
      "           5.7428e-02, -1.7441e-01],\n",
      "         [ 7.9590e-02, -1.3637e-01, -8.5280e-02,  ...,  4.5671e-02,\n",
      "           4.7982e-02, -2.1044e-01]],\n",
      "\n",
      "        [[ 9.7181e-02,  1.4224e-01,  7.6963e-02,  ...,  2.0099e-01,\n",
      "          -2.2105e-01, -7.7940e-02],\n",
      "         [ 1.2157e-01, -3.9740e-02, -7.3070e-02,  ..., -1.7258e-01,\n",
      "           9.8215e-02,  1.1283e-01],\n",
      "         [ 7.7206e-02,  1.1733e-01,  2.7282e-02,  ..., -1.4417e-01,\n",
      "          -3.4644e-02,  2.8070e-01],\n",
      "         [-6.8717e-03,  1.0623e-02, -1.2950e-02,  ..., -1.1519e-01,\n",
      "          -6.7377e-03,  2.6451e-01],\n",
      "         [ 4.6207e-02,  1.7539e-01, -7.3767e-02,  ..., -7.0879e-02,\n",
      "          -2.4141e-02,  1.1212e-01]],\n",
      "\n",
      "        [[ 1.6865e-01,  2.1552e-01,  1.7166e-01,  ...,  1.5892e-01,\n",
      "          -1.9061e-01,  2.3676e-02],\n",
      "         [-4.8515e-02, -3.4270e-02,  5.5046e-02,  ..., -1.4140e-01,\n",
      "           1.2607e-01,  1.6246e-01],\n",
      "         [ 5.3546e-02,  1.2437e-01,  1.6379e-01,  ..., -1.5271e-01,\n",
      "          -1.8866e-02,  2.9478e-01],\n",
      "         [-3.2650e-02,  1.7507e-01,  5.9587e-02,  ..., -1.0610e-01,\n",
      "          -4.7616e-03,  2.0322e-01],\n",
      "         [ 1.1826e-01,  2.0893e-01,  2.5004e-02,  ...,  8.5576e-03,\n",
      "           2.7323e-02,  2.6118e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.3204e-01,  2.4219e-01,  6.6180e-02,  ..., -7.5754e-02,\n",
      "          -1.4364e-01,  1.6087e-01],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 7.2550e-02,  2.6800e-01,  2.0668e-01,  ..., -2.9730e-03,\n",
      "           1.2736e-02,  3.0660e-02]],\n",
      "\n",
      "        [[-2.3173e-02,  2.7423e-01,  1.0705e-01,  ..., -2.9729e-03,\n",
      "           1.2736e-02,  3.0660e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 1.2760e-01,  2.5676e-02,  7.3073e-02,  ...,  5.5219e-02,\n",
      "           5.6426e-02, -2.5936e-01]],\n",
      "\n",
      "        [[ 8.6558e-02,  8.8497e-03, -8.7172e-05,  ...,  5.5219e-02,\n",
      "           5.6426e-02, -2.5936e-01],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]]], grad_fn=<IndexSelectBackward>), tensor([142,  67,  88,  36, 141]))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-116-c1e9d84f1c71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                    \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'passage'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                    \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'question_length'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                    batch['passage_length'])\n\u001b[0m",
      "\u001b[1;32mW:\\conda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-115-35dff7a97088>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, question, passage, question_length, passage_length)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mhidden_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_packed_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0matt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matt_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#X\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0matt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqas_represent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mindice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mW:\\conda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mW:\\conda\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mW:\\conda\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1365\u001b[0m         \u001b[1;33m-\u001b[0m \u001b[0mOutput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0m_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m     \"\"\"\n\u001b[1;32m-> 1367\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1368\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "for batch in batch_generater(dataset, 5):\n",
    "    input('aa')\n",
    "    \n",
    "    model = MJHQAmodel(50, 50,\n",
    "                   torch.tensor(Glove.dic2matrix(Vectorize.question_vocab)),\n",
    "                  torch.tensor(Glove.dic2matrix(Vectorize.passage_vocab)))\n",
    "    indice = model(batch['question'],\n",
    "                   batch['passage'],\n",
    "                   batch['question_length'],\n",
    "                   batch['passage_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vectorize.max_question_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "386.672px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
